\documentclass{article}
\usepackage{enumerate,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
 \geometry{
 letterpaper,
 total={170mm,257mm},
 left=15mm,
 top=15mm,
 }

\title{Group Assignment 2}
\author{Sarah Bailey, Richard Hsia, Lillian Yan Lin, Yue Ma, Will Ruth, Michelle Thiessen}

\newtheorem*{thm1}{Question 3-7}
\newtheorem*{lemma1}{Lemma 1}

\begin{document}
\maketitle
\SweaveOpts{concordance=TRUE}
\begin{enumerate}

\item Question 2.15

To find the density of $Y$
\begin{align*}
F_Y(y) &= P(Y \leq y) \\
       &= P(F(X) \leq y) \\
       &= P(X \leq F^{-1}(y))\\
       &=F_X(F^{-1}(y))\\
       &=y
\end{align*}

Now we take derivative to obtain the density of $Y$.
$$f_Y(y)=\frac{dF_Y(y)}{dy}=\frac{dy}{dy}=1$$
Since $Y=F(X)$, then the $Y\in [0,1]$, so $F$ is uniformly distributed on [0,1].

Now let $U\sim$ Uniform (0,1) and let $X=F_{X}^{-1}(U)$. To show that $X\sim F_{X}(x)$, it must be shown that $P(X\leq x)=F_{X}(x)$
\begin{align*}
P(X\leq x)&=P(F^{-1}_{X}(U)\leq x)\\
          &=P(U\leq F_{X}(x))&\\
          &=F_{X}(x)& &\text{using }P(U\leq a)=a \text{ for } U\sim \text{Unif(0,1)}
\end{align*}

Now we take Uniform (0,1) random variables and generate random variables from an Exponential ($\beta$) distribution. Program is as follow:

<<echo=TRUE, fig=TRUE, include=TRUE, label=uniform>>=
rv_generator<-function(N,beta){
  U<-runif(N,0,1)
  X<--beta*log(1-U)
  #This is the inverse function of ****CDF not pdf**** 
  #of exponential(beta) distribution
  hist(X,freq=F,main="probability density function",
       xlim=c(0,20),xlab="y",ylab="f(y)",breaks=40)
  curve(dexp(x,beta),0,10,add=TRUE,col="red")
  return(X)
}
par(mfrow=c(3,1))
X1<-rv_generator(100,1)
X2<-rv_generator(1000,1)
X3<-rv_generator(10000,1)
@

When N is big enough (such as 10000), we can obtain the ideal result we expected. We can also add Y at the end of the function to obtain rvs we generate

\item Question 3.7

\begin{lemma1}
\label{lem1}
Let $X$ be a random variable with CDF $F$ such that $EX$ exists. Then the following identity holds.
\begin{align*}
\lim_{x \rightarrow \infty} x[1-F(x)] = 0
\end{align*}

\end{lemma1}

\begin{thm1}
	Let $X$ be a continuous random variable with CDF $F$, $p(X>0)=1$ and $EX$ exists. Then the following equality holds.
	\begin{align*}
		EX &= \int^{\infty}_0 [1-F(x)] \, dx\\
	\end{align*}
\end{thm1}

\begin{proof}
This proof uses only elementary calculus, but requires that we assume Lemma 1 without proof. We start by setting up integration by parts as follows.
\begin{align*}
	&u = 1-F(x) && dv = dx\\
	&v = x && du = -f(x) \, dx
\end{align*}
Where $f$ is the pdf of $X$. Then we can re-write the original integral as follows.
\begin{align*}
	\int^{\infty}_0 [1-F(x)] \, dx &= \int^{\infty}_0 u \, dv\\
	&= \left. (uv) \right|_{x=0}^\infty - \int^{\infty}_0 v \, du\\
	&= \left. x[1-F(x)] \right|_{x=0}^\infty + \int^{\infty}_0 x f(x) \, dx\\
	&= \left\{\lim_{x \rightarrow \infty} (x[1-F(x)]) - 0[1-F(0)] \right\} + EX\\
	&= (0 - 0) + EX & \text{(by Lemma 1)}\\
	&= EX
\end{align*}
\end{proof}

\begin{proof}[Alternative Proof]
This proof uses Tonelli's Theorem, but does not require any additional machinery. In particular, it does not require that we assume Lemma 1. We start by re-writing $EX$ as follows.
\begin{align*}
EX &= \int x \, dF(x)\\
&= \int \int_0^x y \, dy \, dF(x)\\
&= \int \int I(0\leq y < x) \, dy \, dF(x)
\end{align*}

Where $I(\cdot)$ is the indicator function. Note that indicator functions are always non-negative, so Tonelli's theorem allows us to exchange the order of integration as follows.
\begin{align*}
EX &= \int \int I(0\leq y < x) \, dy \, dF(x)\\
&= \int \int I(0\leq y < x) \, dF(x) \, dy\\
&= \int \int I(0 \leq y) \cdot I(y < x) \, dF(x) \, dy\\
&= \int I(0 \leq y) \cdot p(X>y) \, dy\\
&= \int_0^\infty [1-F(y)] \, dy
\end{align*}
\end{proof}

\item Question 3.11
\begin{enumerate}
\item

We know $X_n=\sum_{i=1}^{n}Y_i$ and $P(Y_i=-1)=P(Y_i=1)=\frac{1}{2}$. First we find the expectation and variance of $Y_i$.
$$E(Y_i)=(-1)(0.5)+(1)(0.5)=0$$
$$Var(Y_i)=(2)(1)(0.5)-0^2=1$$

Since $Y_i$'s are independent,
$$E(X_n)=E\left(\sum_{i=1}^{n}Y_i\right)=\sum_{i=1}^{n}E(Y_i)=(n)(0)=0$$ $$Var(X_n)=Var\left(\sum_{i=1}^{n}Y_i\right)=\sum_{i=1}^{n}Var(Y_i)=(n)(1)=n$$

\newpage
\item
<<echo=TRUE, fig=TRUE, include=TRUE, label=stock>>=
StockMarket <- function(n,plot=F){
  y <- ifelse(rbinom(n,1,0.5),1,-1)
  Xn <- cumsum(y)
  if(plot)
    plot(1:n,Xn,type="l",xlab="n",ylab="Xn",main="Stock price fluctuation")
  return(sum(y))
}
n <- 10000
par(mfrow=c(2,2))
plots <- replicate(4,StockMarket(n,T))

x <- replicate(10000,StockMarket(n,F))
values <- matrix(c(mean(x),var(x),0,n),nrow=2,ncol=2,byrow=T)
row.names(values) <- c("Simulated","Theoretical")
colnames(values) <- c("Mean of Xn","Variance of Xn")
values
@

Looking at the simulations, we do notice that the graphs are unique, but each follows an overal "up and down" pattern. Although these graphs were all simulated the same way, their patterns are all different. In part (a), we found that $E(X_n)=0$. This means the expectation of the sum, taken across the whole simulation, is zero. Therefore, we expect the average of all the graphs to be $0$. We can see that if we separated the each simulation into $X_n<0$ and $X_n>0$, we would find a similar number of $X_n's$ on both sides of zero. As well, the variance of each simulation should be $n$. The maximum value of $X_n$ across the whole simulation is $n$ and the minimum is $-n$, so the total range is $2n$. In general, we are looking at the sum of all $X_i's$, or the "end result" and not necessarily the pathway the stock price took to reach that point.
\end{enumerate}
\end{enumerate}
\end{document}